{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarkPovoGang/DeepRL/blob/main/DeepRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Project"
      ],
      "metadata": {
        "id": "j0lPQfLxp6EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparation"
      ],
      "metadata": {
        "id": "mpMI7nrpppn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI6nYD7FMUN2",
        "outputId": "b92a14e5-526d-4619-d327-b44c17563da9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-qbsxsfa1\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-qbsxsfa1\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a9b1bf5920416aaeaec965c25dd9e8f98c864f16\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.6.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369370 sha256=ac62d72b7f827cbda353dd98a11b0532ece08e8a4c393553ca1dc0ae9c7d845f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-orwnpj_n/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "import clip\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "model, preprocess = clip.load(\"RN50\")\n",
        "model.cuda().eval()\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n",
        "\n",
        "preprocess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYzz611UOQg4",
        "outputId": "e41f94f8-b78b-4f0c-f400-4d218c76245d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.0.1+cu118\n",
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7fedc237cd30>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import gdown\n",
        "import tarfile\n",
        "from PIL import Image #, ImageDraw\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class RefCOCOg:\n",
        "    FILE_ID = \"1wyyksgdLwnRMC9pQ-vjJnNUn47nWhyMD\"\n",
        "    ARCHIVE_NAME = \"refcocog.tar.gz\"\n",
        "    NAME = \"refcocog\"\n",
        "    ANNOTATIONS = \"annotations/refs(umd).p\"\n",
        "    JSON = \"annotations/instances.json\"\n",
        "    IMAGES = \"images\"\n",
        "    IMAGE_NAME = \"COCO_train2014_{}.jpg\"\n",
        "\n",
        "    def __init__(self, data_dir, split, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self._check_dataset()\n",
        "        self.split = split\n",
        "        self._filter_annotation(\n",
        "            os.path.join(self.data_dir, self.NAME, self.ANNOTATIONS)\n",
        "        )\n",
        "        self._load_json()\n",
        "        self.transform = transform\n",
        "\n",
        "    def _check_dataset(self):\n",
        "        if not os.path.exists(os.path.join(self.data_dir, self.ARCHIVE_NAME)):\n",
        "            if not os.path.exists(self.data_dir):\n",
        "                os.mkdir(self.data_dir)\n",
        "            print(\"Downloading dataset...\")\n",
        "            gdown.download(id=self.FILE_ID)\n",
        "        if not os.path.exists(os.path.join(self.data_dir, self.NAME)):\n",
        "            print(\"Extracting dataset...\")\n",
        "            with tarfile.open(\n",
        "                os.path.join(self.data_dir, self.ARCHIVE_NAME), \"r:gz\"\n",
        "            ) as tar:\n",
        "                tar.extractall(path=self.data_dir)\n",
        "        else:\n",
        "            print(\"Dataset already extracted\")\n",
        "\n",
        "    def _load_json(self):\n",
        "        with open(os.path.join(self.data_dir, self.NAME, self.JSON)) as f:\n",
        "            self.json = json.load(f)\n",
        "        self.json = pd.DataFrame(self.json[\"annotations\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotation)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get line by index\n",
        "        raw = self.annotation.iloc[idx]\n",
        "        # get image\n",
        "        image = self._get_image(raw)\n",
        "        # get sentences\n",
        "        sentences = self._get_sentences(raw)\n",
        "        # get bbox\n",
        "        bbox = self._get_bbox(raw)\n",
        "\n",
        "        return self._get_vector(image, sentences, bbox)\n",
        "\n",
        "    def _get_image(self, raw):\n",
        "        # get image_id\n",
        "        image_id = raw[\"image_id\"]\n",
        "        # pad image_id to 12 digits\n",
        "        image_id = str(image_id).zfill(12)\n",
        "        # convert image to tensor\n",
        "        image = Image.open(\n",
        "            os.path.join(\n",
        "                self.data_dir, self.NAME, self.IMAGES, self.IMAGE_NAME.format(image_id)\n",
        "            )\n",
        "        )\n",
        "        return image\n",
        "\n",
        "    def _get_sentences(self, raw):\n",
        "        # get sentences\n",
        "        sentences = raw[\"sentences\"]\n",
        "        # get raw sentences\n",
        "        sentences = [sentence[\"raw\"] for sentence in sentences]\n",
        "        return sentences\n",
        "\n",
        "    def _get_bbox(self, raw):\n",
        "        # get ref_id\n",
        "        id = raw[\"ann_id\"]\n",
        "        bbox = self.json[self.json[\"id\"] == id][\"bbox\"].values[0]\n",
        "        return bbox\n",
        "\n",
        "    def _filter_annotation(self, path):\n",
        "        self.annotation = pd.read_pickle(path)\n",
        "        self.annotation = pd.DataFrame(self.annotation)\n",
        "        self.annotation = self.annotation[self.annotation[\"split\"] == self.split]\n",
        "\n",
        "    def _get_vector(self, image, sentences, bbox):\n",
        "        image = preprocess(image).unsqueeze(0).to(device)\n",
        "        text = clip.tokenize(sentences).to(device)\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(image)\n",
        "            text_features = model.encode_text(text)\n",
        "\n",
        "        bbox = torch.tensor(bbox).unsqueeze(0).to(device)\n",
        "\n",
        "        print(f\"Image shape: {image_features.shape}, Text shape: {text_features.shape}, Bbox shape: {bbox.shape}\")\n",
        "\n",
        "        # Combine image and text features and normalize\n",
        "        # product = np.multiply(image_features.cpu(), text_features.cpu())\n",
        "        # out = np.divide(product, np.linalg.norm(product, axis=1).reshape(-1, 1))\n",
        "        product = torch.mul(image_features, text_features)\n",
        "        out = torch.div(product, torch.norm(product, dim=1).reshape(-1, 1))\n",
        "\n",
        "\n",
        "        bboxes = bbox.repeat(text_features.shape[0],1)\n",
        "        print(bboxes)\n",
        "        # append bbox\n",
        "        out = torch.cat((out, bboxes), dim=1)\n",
        "\n",
        "\n",
        "\n",
        "        # append bbox\n",
        "        # print(\"shape\",product.shape)\n",
        "        # out = np.append(out, bbox.cpu(), axis=1)\n",
        "        print(f\"Output shape: {out.shape}\")\n",
        "        return out"
      ],
      "metadata": {
        "id": "afK1s3XLMl1d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = RefCOCOg('.', 'val')\n",
        "for i in range(5):\n",
        "  x = dataset[i]\n",
        "  print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQkhbZucR7eZ",
        "outputId": "da9313ce-1c8c-41e1-c69b-8c2c9b4fc254"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset already extracted\n",
            "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
            "tensor([[285.0400,  23.2900, 139.7400, 123.1100],\n",
            "        [285.0400,  23.2900, 139.7400, 123.1100]], device='cuda:0')\n",
            "Output shape: torch.Size([2, 1028])\n",
            "tensor([[ 3.9077e-04,  1.1349e-03,  3.8624e-03,  ...,  2.3290e+01,\n",
            "          1.3974e+02,  1.2311e+02],\n",
            "        [-7.3051e-04,  2.1496e-03,  3.9139e-03,  ...,  2.3290e+01,\n",
            "          1.3974e+02,  1.2311e+02]], device='cuda:0')\n",
            "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
            "tensor([[183.4000,  68.3400, 126.8100,  98.8100],\n",
            "        [183.4000,  68.3400, 126.8100,  98.8100]], device='cuda:0')\n",
            "Output shape: torch.Size([2, 1028])\n",
            "tensor([[-2.8057e-03, -2.8992e-04, -6.5956e-03,  ...,  6.8340e+01,\n",
            "          1.2681e+02,  9.8810e+01],\n",
            "        [-1.7061e-03, -4.1056e-04, -6.4087e-03,  ...,  6.8340e+01,\n",
            "          1.2681e+02,  9.8810e+01]], device='cuda:0')\n",
            "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
            "tensor([[394.1600, 269.7600, 237.4600, 117.2900],\n",
            "        [394.1600, 269.7600, 237.4600, 117.2900]], device='cuda:0')\n",
            "Output shape: torch.Size([2, 1028])\n",
            "tensor([[-1.0700e-03,  2.7585e-04,  4.5300e-04,  ...,  2.6976e+02,\n",
            "          2.3746e+02,  1.1729e+02],\n",
            "        [-1.0290e-03, -4.8757e-04, -2.1057e-03,  ...,  2.6976e+02,\n",
            "          2.3746e+02,  1.1729e+02]], device='cuda:0')\n",
            "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024]), Bbox shape: torch.Size([1, 4])\n",
            "tensor([[ 90.9500, 158.2600, 196.1000, 152.0500]], device='cuda:0')\n",
            "Output shape: torch.Size([1, 1028])\n",
            "tensor([[ 1.4334e-03,  1.3161e-03, -2.2011e-03,  ...,  1.5826e+02,\n",
            "          1.9610e+02,  1.5205e+02]], device='cuda:0')\n",
            "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024]), Bbox shape: torch.Size([1, 4])\n",
            "tensor([[ 86.2600, 112.8500, 172.9300, 123.0000]], device='cuda:0')\n",
            "Output shape: torch.Size([1, 1028])\n",
            "tensor([[-4.3449e-03,  8.6737e-04, -7.8964e-03,  ...,  1.1285e+02,\n",
            "          1.7293e+02,  1.2300e+02]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6VYv9k9LdUX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our approach"
      ],
      "metadata": {
        "id": "ZKroxpH0qnrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl swig x11-utils\n",
        "!pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
      ],
      "metadata": {
        "id": "_VKW0jTYc4LR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f410a60-8f3c-422f-843c-cedf8af28caf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 python2 python2-minimal swig4.0 x11-xkb-utils xfonts-base\n",
            "  xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  python-tk python-numpy libgle3 python2-doc swig-doc swig-examples\n",
            "  swig4.0-examples swig4.0-doc mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 libfontenc1 libpython2-stdlib libxfont2 libxkbfile1 libxtst6\n",
            "  libxxf86dga1 python-opengl python2 python2-minimal swig swig4.0 x11-utils\n",
            "  x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 19 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 9,627 kB of archives.\n",
            "After this operation, 24.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2-minimal amd64 2.7.17-2ubuntu4 [27.5 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 libpython2-stdlib amd64 2.7.17-2ubuntu4 [7,072 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 python2 amd64 2.7.17-2ubuntu4 [26.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 freeglut3 amd64 2.8.1-3 [73.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 python-opengl all 3.1.0+dfsg-2build1 [486 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig4.0 amd64 4.0.1-5build1 [1,081 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal/universe amd64 swig all 4.0.1-5build1 [5,528 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 9,627 kB in 1s (10.5 MB/s)\n",
            "Selecting previously unselected package python2-minimal.\n",
            "(Reading database ... 123069 files and directories currently installed.)\n",
            "Preparing to unpack .../python2-minimal_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package libpython2-stdlib:amd64.\n",
            "Preparing to unpack .../libpython2-stdlib_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up python2-minimal (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package python2.\n",
            "(Reading database ... 123098 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python2_2.7.17-2ubuntu4_amd64.deb ...\n",
            "Unpacking python2 (2.7.17-2ubuntu4) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "Preparing to unpack .../01-freeglut3_2.8.1-3_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-3) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../02-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../03-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../04-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../05-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../06-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Selecting previously unselected package python-opengl.\n",
            "Preparing to unpack .../07-python-opengl_3.1.0+dfsg-2build1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../08-swig4.0_4.0.1-5build1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.1-5build1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../09-swig_4.0.1-5build1_all.deb ...\n",
            "Unpacking swig (4.0.1-5build1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../10-x11-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../11-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../12-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../13-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../14-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../15-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../16-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-3) ...\n",
            "Setting up libpython2-stdlib:amd64 (2.7.17-2ubuntu4) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Setting up python2 (2.7.17-2ubuntu4) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up swig4.0 (4.0.1-5build1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up swig (4.0.1-5build1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-2build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up x11-utils (7.7+5) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Collecting gym[box2d]==0.17.*\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.10/dist-packages (3.1.7)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "  Downloading PyOpenGL_accelerate-3.1.7-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.17.*) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]==0.17.*) (1.22.4)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym[box2d]==0.17.*)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle<1.7.0,>=1.2.0 (from gym[box2d]==0.17.*)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting box2d-py~=2.3.5 (from gym[box2d]==0.17.*)\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting EasyProcess (from pyvirtualdisplay==0.2.*)\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.18.3)\n",
            "Building wheels for collected packages: box2d-py, gym\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp310-cp310-linux_x86_64.whl size=2784113 sha256=d45df6979d5da4da560ffd6773219c1006442b2a46e037a06d26db0385605da4\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/01/d2/6a780da77ccb98b1d2facdd520a8d10838a03b590f6f8d50c0\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654622 sha256=6d65945fa9631c91a482662c3ecbd07c12b5f826dfccc719daf9a4f1e1828353\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/4b/74/fcfc8238472c34d7f96508a63c962ff3ac9485a9a4137afd4e\n",
            "Successfully built box2d-py gym\n",
            "Installing collected packages: PyOpenGL-accelerate, EasyProcess, box2d-py, pyvirtualdisplay, pyglet, cloudpickle, gym\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed EasyProcess-1.1 PyOpenGL-accelerate-3.1.7 box2d-py-2.3.8 cloudpickle-1.6.0 gym-0.17.3 pyglet-1.5.0 pyvirtualdisplay-0.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import spaces\n",
        "import numpy as np\n",
        "test = spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([499, 299, 500, 300]), dtype=int)\n",
        "# test = spaces.Box([500, 300, 500, 300], dtype=int)\n",
        "for i in range(10):\n",
        "  print(test.sample())"
      ],
      "metadata": {
        "id": "hvvaS0iDl4cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce39f6e2-bb44-4730-d7c9-a7db2f2eb712"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[286 289 279 223]\n",
            "[ 31 188 313 121]\n",
            "[376  58  77 185]\n",
            "[291 275 390 179]\n",
            "[276 247  16  82]\n",
            "[ 11 166  98  12]\n",
            "[ 75 166 434  23]\n",
            "[ 76 248 475 120]\n",
            "[ 86 249 154 240]\n",
            "[220  58 211  32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "# class syntax\n",
        "class actions(Enum):\n",
        "  ACT_RT = 0 #Right\n",
        "  ACT_LT = 1 #Left\n",
        "  ACT_UP = 2 #Up\n",
        "  ACT_DN = 3 #Down\n",
        "  ACT_TA = 4 #Taller\n",
        "  ACT_FA = 5 #Fatter\n",
        "  ACT_SR = 6 #Shorter\n",
        "  ACT_TH = 7 #Thiner\n",
        "  ACT_TR = 8 #Trigger\n"
      ],
      "metadata": {
        "id": "k_kBbwDrR8cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "class VisualGroundingEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "    def __init__(self, width, height, move_factor=0.2, scale_factor=0.1, render_mode=None):\n",
        "        self.width = width  # The width of the image\n",
        "        self.height = height  # The height of the image\n",
        "        self.window_size = 512  # The size of the PyGame window\n",
        "        self.move_factor = move_factor\n",
        "        self.scale_factor = scale_factor\n",
        "\n",
        "        # Observations are dictionaries with the agent's and the target's location.\n",
        "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"agent\": spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([self.width-1, self.height, self.width, self.height]), dtype=int),\n",
        "                \"target\": spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([self.width-1, self.height, self.width, self.height]), dtype=int),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # We have 9 actions, corresponding to \"right\", \"up\", \"left\", \"down\", \"v-shrink\", \"v-stretch\", \"h-shrink\", \"h-stretch\", \"confirm\"\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "\n",
        "        \"\"\"\n",
        "        The following dictionary maps abstract actions from `self.action_space` to\n",
        "        the direction we will walk in if that action is taken.\n",
        "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
        "        \"\"\"\n",
        "\n",
        "        #TODO: later when current data is available\n",
        "        self._action_to_direction = {\n",
        "            0: np.array([1, 0]),\n",
        "            1: np.array([0, 1]),\n",
        "            2: np.array([-1, 0]),\n",
        "            3: np.array([0, -1]),\n",
        "        }\n",
        "\n",
        "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        \"\"\"\n",
        "        If human-rendering is used, `self.window` will be a reference\n",
        "        to the window that we draw to. `self.clock` will be a clock that is used\n",
        "        to ensure that the environment is rendered at the correct framerate in\n",
        "        human-mode. They will remain `None` until human-mode is used for the\n",
        "        first time.\n",
        "        \"\"\"\n",
        "        self.window = None\n",
        "        self.clock = None\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
        "\n",
        "    def _get_info(self):\n",
        "        # return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
        "        # TODO: maybe return current history of movement\n",
        "        pass\n",
        "\n",
        "    def reset(self, true_bbox: np.array, seed=None, options=None):\n",
        "        self.x1 = 0\n",
        "        self.y1 = 0\n",
        "        self.bbox_width = self.width\n",
        "        self.bbox_height = self.height\n",
        "\n",
        "        # We need the following line to seed self.np_random\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        # Choose the agent's location uniformly at random\n",
        "        self._agent_location = np.array([0,0, self.width, self.height])\n",
        "\n",
        "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
        "        self._target_location = #TODO: init with true bbox\n",
        "\n",
        "        observation = self._get_obs()\n",
        "        info = self._get_info()\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            self._render_frame()\n",
        "\n",
        "        return observation, info\n",
        "\n",
        "    def _update_bbox(self, action):\n",
        "      ALPHA = 0.2\n",
        "      BETA  = 0.1\n",
        "      x2 = self.x1 + self.bbox_width\n",
        "      y2 = self.y1 + self.bbox_height\n",
        "      assert action >= actions.ACT_RT and action <= actions.ACT_TR\n",
        "      self.action_history.append(action)\n",
        "\n",
        "      if action <= actions.ACT_DN:\n",
        "        delta_w = int(ALPHA * self.bbox_width())\n",
        "        delta_h = int(ALPHA * self.bbox_height())\n",
        "      else:\n",
        "        delta_w = int(BETA * self.bbox_width())\n",
        "        delta_h = int(BETA * self.bbox_height())\n",
        "\n",
        "      # PREVENT_STUCK:\n",
        "      if (delta_h == 0):\n",
        "        delta_h = 1\n",
        "      if (delta_w == 0):\n",
        "        delta_w = 1\n",
        "\n",
        "      #Do the corresponding action to the window\n",
        "      if action == actions.ACT_RT:\n",
        "        self.x1 += delta_w\n",
        "        x2 += delta_w\n",
        "      elif action == actions.ACT_LT:\n",
        "        self.x1 -= delta_w\n",
        "        x2 -= delta_w\n",
        "      elif action == actions.ACT_UP:\n",
        "        self.y1 -= delta_h\n",
        "        y2 -= delta_h\n",
        "      elif action == actions.ACT_DN:\n",
        "        self.y1 += delta_h\n",
        "        y2 += delta_h\n",
        "      elif action == actions.ACT_TA:\n",
        "        self.y1 -= delta_h\n",
        "        y2 += delta_h\n",
        "      elif action == actions.ACT_FA:\n",
        "        self.x1 -= delta_w\n",
        "        x2 += delta_w\n",
        "      elif action == actions.ACT_SR:\n",
        "        self.y1 += delta_h\n",
        "        y2 -= delta_h\n",
        "      elif action == actions.ACT_TH:\n",
        "        self.x1 += delta_w\n",
        "        x2 -= delta_w\n",
        "      elif action == actions.ACT_TR:\n",
        "        pass\n",
        "      else:\n",
        "        raise NotImplemented\n",
        "\n",
        "      # ensure bbox inside image\n",
        "      if self.x1 < 0:\n",
        "        self.x1 = 0\n",
        "      if self.y1 < 0:\n",
        "        self.y1 = 0\n",
        "      if self.x2 >= self.image_width:\n",
        "        self.x2 = self.image_width - 1\n",
        "      if self.y2 >= self.image_height:\n",
        "        self.y2 = self.image_height - 1\n",
        "      # ret x,y,w,h\n",
        "      return  self.x1, self.y1, x2-self.x1, y2-self.y1\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "            # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
        "            self._agent_location = self._update_bbox(action)\n",
        "            # An episode is done iff the agent has reached the target\n",
        "            terminated = np.array_equal(self._agent_location, self._target_location) #TODO: or quite close\n",
        "            reward = 1 if terminated else 0  # TODO: change reward\n",
        "            observation = self._get_obs()\n",
        "            info = self._get_info()\n",
        "\n",
        "            if self.render_mode == \"human\":\n",
        "                self._render_frame()\n",
        "\n",
        "            return observation, reward, terminated, False, info\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            return self._render_frame()\n",
        "\n",
        "    def _render_frame(self):\n",
        "        if self.window is None and self.render_mode == \"human\":\n",
        "            pygame.init()\n",
        "            pygame.display.init()\n",
        "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        if self.clock is None and self.render_mode == \"human\":\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
        "        canvas.fill((255, 255, 255))\n",
        "        pix_square_size = (\n",
        "            self.window_size / self.size\n",
        "        )  # The size of a single grid square in pixels\n",
        "\n",
        "        # First we draw the target\n",
        "        pygame.draw.rect(\n",
        "            canvas,\n",
        "            (255, 0, 0),\n",
        "            pygame.Rect(\n",
        "                pix_square_size * self._target_location,\n",
        "                (pix_square_size, pix_square_size),\n",
        "            ),\n",
        "        )\n",
        "        # Now we draw the agent\n",
        "        pygame.draw.circle(\n",
        "            canvas,\n",
        "            (0, 0, 255),\n",
        "            (self._agent_location + 0.5) * pix_square_size,\n",
        "            pix_square_size / 3,\n",
        "        )\n",
        "\n",
        "        # Finally, add some gridlines\n",
        "        for x in range(self.size + 1):\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (0, pix_square_size * x),\n",
        "                (self.window_size, pix_square_size * x),\n",
        "                width=3,\n",
        "            )\n",
        "            pygame.draw.line(\n",
        "                canvas,\n",
        "                0,\n",
        "                (pix_square_size * x, 0),\n",
        "                (pix_square_size * x, self.window_size),\n",
        "                width=3,\n",
        "            )\n",
        "\n",
        "        if self.render_mode == \"human\":\n",
        "            # The following line copies our drawings from `canvas` to the visible window\n",
        "            self.window.blit(canvas, canvas.get_rect())\n",
        "            pygame.event.pump()\n",
        "            pygame.display.update()\n",
        "\n",
        "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
        "            # The following line will automatically add a delay to keep the framerate stable.\n",
        "            self.clock.tick(self.metadata[\"render_fps\"])\n",
        "        else:  # rgb_array\n",
        "            return np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        if self.window is not None:\n",
        "            pygame.display.quit()\n",
        "            pygame.quit()"
      ],
      "metadata": {
        "id": "DU5sU5s2cc24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "7bf756ed-8186-448b-ee0e-baebcf6fcfe1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3328d72bc599>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    self._target_location = #TODO: init with true bbox\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "register(\n",
        "    id='DeepLearningProject/VisualGrounding-v0',\n",
        "    entry_point='DeepLearningProject.envs:VisualGrounding',\n",
        "    max_episode_steps=300,\n",
        ")"
      ],
      "metadata": {
        "id": "Xfi4D9z39mA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: move everything in a repo, but leave here for knowledge\n",
        "\n",
        "from setuptools import setup\n",
        "\n",
        "setup(\n",
        "    name=\"DeepLearningProject\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=[\"gym==0.26.0\", \"pygame==2.1.0\"],\n",
        ")"
      ],
      "metadata": {
        "id": "EeSH5dic9zqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym_examples\n",
        "env = gym.make('DeepLearningProject/VisualGrounding-v0')"
      ],
      "metadata": {
        "id": "KRUOg-oe-Ig5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network, Agent and Utils"
      ],
      "metadata": {
        "id": "MKRUOZc6baAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "31mKYCXhZyZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: change\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_features, n_actions, features=24):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        # multi layer perceptron\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_features, features),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features, features * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features * 2, features * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features * 4, features * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features * 2, features),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(features, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "CxCoa4TUZ0L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a subclass of Tuple with named attributes representing experience\n",
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity):\n",
        "\n",
        "        # represent the buffer as a deque\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "\n",
        "        # add the current experience to the buffer\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        # sample an index for each element in the batch\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "\n",
        "        # extract experience entries for each element in the batch\n",
        "        # each value returned by zip is a list of length batch_size\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "\n",
        "        # return results as numpy arrays\n",
        "        return np.array(states), \\\n",
        "               np.array(actions), \\\n",
        "               np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), \\\n",
        "               np.array(next_states)"
      ],
      "metadata": {
        "id": "Sd7GcGt6Z4kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "\n",
        "        # restarts the environment and reset the accumulated reward\n",
        "        self.state = self.env.reset().astype(np.float32)\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        # no need to create a computational graph when gathering experience\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # will contain the total reward for the episode if the episode ends\n",
        "            # or None otherwise\n",
        "            done_reward = None\n",
        "\n",
        "            # sample the action randomly with probability epsilon\n",
        "            if np.random.random() < epsilon:\n",
        "                action = self.env.action_space.sample()\n",
        "\n",
        "            # otherwise, select action based on qvalues\n",
        "            else:\n",
        "\n",
        "                # creates a batch made of a single state\n",
        "                # state_a = np.array([self.state], copy=False)\n",
        "                state_tensor = torch.tensor(self.state).unsqueeze(0).to(device)\n",
        "\n",
        "                # get qvalues and select the index of the maximum\n",
        "                q_values = net(state_tensor)\n",
        "                _, selected_action = torch.max(q_values, dim=1)\n",
        "                action = int(selected_action.item())\n",
        "\n",
        "            # perform a step in the environment\n",
        "            new_state, reward, is_done, _ = self.env.step(action)\n",
        "            new_state = new_state.astype(np.float32)\n",
        "            self.total_reward += reward\n",
        "\n",
        "            # save the new experience\n",
        "            exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "            self.exp_buffer.append(exp)\n",
        "\n",
        "            # registers the current state\n",
        "            self.state = new_state\n",
        "\n",
        "            # Gets the current representation of the environment\n",
        "            # current_rgb_image = self.env.render(mode='rgb_array')\n",
        "\n",
        "            # if the episode is finished, reset the environment\n",
        "            if is_done:\n",
        "                done_reward = self.total_reward\n",
        "                self._reset()\n",
        "\n",
        "            return done_reward #, current_rgb_image"
      ],
      "metadata": {
        "id": "MomYFhu3Z6fX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch, net, target_net, device=\"cpu\"):\n",
        "\n",
        "    # unpack the batch\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    #states_v = torch.tensor(np.array(states, copy=False)).to(device)\n",
        "    #next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)\n",
        "\n",
        "    # transform the batch elements to tensors\n",
        "    states_v = torch.from_numpy(states).to(device)\n",
        "    next_states_v = torch.from_numpy(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "    # infer the qvalues for the states\n",
        "    state_qvalues = net(states_v)\n",
        "\n",
        "    # extract the qvalues for the action that was selected\n",
        "    state_action_qvalues = state_qvalues.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # compute the qvalues for the next states using the target DQN\n",
        "        next_state_qvalues = target_net(next_states_v)\n",
        "\n",
        "        # extract the maximum one\n",
        "        next_state_max_qvalue = next_state_qvalues.max(dim=1)[0]\n",
        "\n",
        "        # if the next state refers to an ended episode, it has no value\n",
        "        next_state_max_qvalue[done_mask] = 0.0\n",
        "\n",
        "        next_state_max_qvalue = next_state_max_qvalue.detach()\n",
        "\n",
        "    # Computes the expected qvalue using the Bellman equation\n",
        "    expected_state_action_qvalues = rewards_v + GAMMA * next_state_max_qvalue\n",
        "\n",
        "    # Penalizes the DQN for inferring a qvalue different from the one\n",
        "    # computed with the target DQN using the Bellman equation\n",
        "    return nn.MSELoss()(state_action_qvalues, expected_state_action_qvalues)"
      ],
      "metadata": {
        "id": "CJQe0pCDZ8gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop and Utils"
      ],
      "metadata": {
        "id": "mMWB3mAZcIaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_ENV_NAME = \"\"\n",
        "\n",
        "# we terminate training if the model on average balances the pole\n",
        "# for at least 195 steps\n",
        "MEAN_REWARD_BOUND = 195\n",
        "\n",
        "# discount factor\n",
        "GAMMA = 0.99\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# size of the replay buffer\n",
        "REPLAY_SIZE = 10000\n",
        "\n",
        "# warmup frames for the replay buffer\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# frequency for transferring weights from the actor DQN to the target DQN\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "\n",
        "# epsilon\n",
        "EPSILON_DECAY_LAST_FRAME = 15000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "def train(net, target_net, env, buffer, agent, device, writer):\n",
        "\n",
        "    # epsilon starts from the initial value and is then annealed\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    # instantiate the optimizer. Note that target_net is not optimized\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    total_rewards = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    # frame idx and time at which the last episode ended\n",
        "    ts_frame = 0\n",
        "    ts = time.time()\n",
        "    best_m_reward = None\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "\n",
        "        # compute the current epsilon with linear annealing\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "        # perform a step in the environment to gather experience\n",
        "        # reward, rbg_image = agent.play_step(net, epsilon, device=device)\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "\n",
        "        # if the current episode has ended\n",
        "        if reward is not None:\n",
        "\n",
        "            # register the current total reward\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "            # compute training speed\n",
        "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "            ts_frame = frame_idx\n",
        "            ts = time.time()\n",
        "\n",
        "            # compute the mean reward over the last 100 episodes\n",
        "            m_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"%d: done %d games, reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), m_reward, epsilon, speed\n",
        "            ))\n",
        "\n",
        "            # log values\n",
        "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "            writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "            # update best rewards\n",
        "            if best_m_reward is None or best_m_reward < m_reward:\n",
        "                #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "                if best_m_reward is not None:\n",
        "                    print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
        "\n",
        "                best_m_reward = m_reward\n",
        "\n",
        "            # stop training when a certain reward is achieved\n",
        "            if m_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        # continue to collect experience until the warmup finishes\n",
        "        if len(buffer) < REPLAY_START_SIZE:\n",
        "            continue\n",
        "\n",
        "        # at regular intervals load the weights of the actor DQN into the target DQN\n",
        "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        # perform an optimization step\n",
        "        optimizer.zero_grad()\n",
        "        batch = buffer.sample(BATCH_SIZE)\n",
        "        loss_t = calc_loss(batch, net, target_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "cLizKhHVZ-Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_episode(net, env, agent, device, writer):\n",
        "\n",
        "    from IPython import display as ipythondisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    display = Display(visible=0, size=(400, 300))\n",
        "    display.start()\n",
        "\n",
        "    # reset the environment\n",
        "    agent._reset()\n",
        "    all_frames = []\n",
        "\n",
        "    reward = None\n",
        "\n",
        "    # play an episode\n",
        "    while reward is None:\n",
        "\n",
        "        reward, rgb_image = agent.play_step(net, epsilon=0.0, device=device)\n",
        "\n",
        "        # change fromat from (H, W, C) to (C, H, W) and saves the image\n",
        "        rgb_image = torch.from_numpy(np.copy(rgb_image)).permute(2, 0, 1)\n",
        "        all_frames.append(rgb_image)\n",
        "\n",
        "    # video must be put into (batch, time, C, H, W) format to be saved\n",
        "    video = torch.stack(all_frames, dim=0).unsqueeze(0)\n",
        "    # save the video\n",
        "    writer.add_video(\"sample_episode\", video, global_step=0, fps=10)"
      ],
      "metadata": {
        "id": "GHidO2pRaAHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "    # build the environment\n",
        "    env = gym.make(DEFAULT_ENV_NAME)\n",
        "\n",
        "    # create actor and target DQN models\n",
        "    net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "    target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
        "\n",
        "    # initialize the logger\n",
        "    writer = SummaryWriter(log_dir=\"runs\")\n",
        "    print(net)\n",
        "\n",
        "    # instantiate the experience buffer and the agent that collects experience\n",
        "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "    agent = Agent(env, buffer)\n",
        "\n",
        "    # train the network\n",
        "    train(net, target_net, env, buffer, agent, device, writer)\n",
        "\n",
        "    # save a sample episode\n",
        "    # save_episode(net, env, agent, device, writer)\n",
        "\n",
        "    writer.close()\n"
      ],
      "metadata": {
        "id": "9shkETtGaFl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r runs"
      ],
      "metadata": {
        "id": "MCTbsP1PaIA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "5FZ2VopqaJzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "gDDKIPXZaOjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.figure(figsize=(10, 10))\n",
        "# plt.subplot(2, 2, 1)\n",
        "# plt.plot(logs[\"reward\"])\n",
        "# plt.title(\"training rewards (average)\")\n",
        "# plt.subplot(2, 2, 2)\n",
        "# plt.plot(logs[\"step_count\"])\n",
        "# plt.title(\"Max step count (training)\")\n",
        "# plt.subplot(2, 2, 3)\n",
        "# plt.plot(logs[\"eval reward (sum)\"])\n",
        "# plt.title(\"Return (test)\")\n",
        "# plt.subplot(2, 2, 4)\n",
        "# plt.plot(logs[\"eval step_count\"])\n",
        "# plt.title(\"Max step count (test)\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "_CU0-J0Tt-qY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}