{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DarkPovoGang/DeepRL/blob/main/DeepRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0lPQfLxp6EU"
   },
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpMI7nrpppn2"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYzz611UOQg4",
    "outputId": "e41f94f8-b78b-4f0c-f400-4d218c76245d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.0.1+cu117\n",
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f7d536c3880>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "import clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "model, preprocess = clip.load(\"RN50\")\n",
    "model.eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "afK1s3XLMl1d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gdown\n",
    "import tarfile\n",
    "from PIL import Image #, ImageDraw\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class RefCOCOg:\n",
    "    FILE_ID = \"1wyyksgdLwnRMC9pQ-vjJnNUn47nWhyMD\"\n",
    "    ARCHIVE_NAME = \"refcocog.tar.gz\"\n",
    "    NAME = \"refcocog\"\n",
    "    ANNOTATIONS = \"annotations/refs(umd).p\"\n",
    "    JSON = \"annotations/instances.json\"\n",
    "    IMAGES = \"images\"\n",
    "    IMAGE_NAME = \"COCO_train2014_{}.jpg\"\n",
    "\n",
    "    def __init__(self, data_dir, split, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self._check_dataset()\n",
    "        self.split = split\n",
    "        self._filter_annotation(\n",
    "            os.path.join(self.data_dir, self.NAME, self.ANNOTATIONS)\n",
    "        )\n",
    "        self._load_json()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _check_dataset(self):\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.ARCHIVE_NAME)):\n",
    "            if not os.path.exists(self.data_dir):\n",
    "                os.mkdir(self.data_dir)\n",
    "            print(\"Downloading dataset...\")\n",
    "            gdown.download(id=self.FILE_ID)\n",
    "        if not os.path.exists(os.path.join(self.data_dir, self.NAME)):\n",
    "            print(\"Extracting dataset...\")\n",
    "            with tarfile.open(\n",
    "                os.path.join(self.data_dir, self.ARCHIVE_NAME), \"r:gz\"\n",
    "            ) as tar:\n",
    "                tar.extractall(path=self.data_dir)\n",
    "        else:\n",
    "            print(\"Dataset already extracted\")\n",
    "\n",
    "    def _load_json(self):\n",
    "        with open(os.path.join(self.data_dir, self.NAME, self.JSON)) as f:\n",
    "            self.json = json.load(f)\n",
    "        self.json = pd.DataFrame(self.json[\"annotations\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get line by index\n",
    "        raw = self.annotation.iloc[idx]\n",
    "        # get image\n",
    "        image = self._get_image(raw)\n",
    "        # get sentences\n",
    "        sentences = self._get_sentences(raw)\n",
    "        # get bbox\n",
    "        bbox = self._get_bbox(raw)\n",
    "\n",
    "        return self._get_vector(image, sentences, bbox)\n",
    "\n",
    "    def _get_image(self, raw):\n",
    "        # get image_id\n",
    "        image_id = raw[\"image_id\"]\n",
    "        # pad image_id to 12 digits\n",
    "        image_id = str(image_id).zfill(12)\n",
    "        # convert image to tensor\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.data_dir, self.NAME, self.IMAGES, self.IMAGE_NAME.format(image_id)\n",
    "            )\n",
    "        )\n",
    "        return image\n",
    "\n",
    "    def _get_sentences(self, raw):\n",
    "        # get sentences\n",
    "        sentences = raw[\"sentences\"]\n",
    "        # get raw sentences\n",
    "        sentences = [sentence[\"raw\"] for sentence in sentences]\n",
    "        return sentences\n",
    "\n",
    "    def _get_bbox(self, raw):\n",
    "        # get ref_id\n",
    "        id = raw[\"ann_id\"]\n",
    "        bbox = self.json[self.json[\"id\"] == id][\"bbox\"].values[0]\n",
    "        return bbox\n",
    "\n",
    "    def _filter_annotation(self, path):\n",
    "        self.annotation = pd.read_pickle(path)\n",
    "        self.annotation = pd.DataFrame(self.annotation)\n",
    "        self.annotation = self.annotation[self.annotation[\"split\"] == self.split]\n",
    "\n",
    "    def _get_vector(self, image, sentences, bbox):\n",
    "        image = preprocess(image).unsqueeze(0).to(device)\n",
    "        text = clip.tokenize(sentences).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text)\n",
    "\n",
    "        bbox = torch.tensor(bbox).unsqueeze(0).to(device)\n",
    "\n",
    "        print(f\"Image shape: {image_features.shape}, Text shape: {text_features.shape}, Bbox shape: {bbox.shape}\")\n",
    "\n",
    "        # Combine image and text features and normalize\n",
    "        # product = np.multiply(image_features.cpu(), text_features.cpu())\n",
    "        # out = np.divide(product, np.linalg.norm(product, axis=1).reshape(-1, 1))\n",
    "        product = torch.mul(image_features, text_features)\n",
    "        out = torch.div(product, torch.norm(product, dim=1).reshape(-1, 1))\n",
    "\n",
    "\n",
    "        bboxes = bbox.repeat(text_features.shape[0],1)\n",
    "        print(bboxes)\n",
    "        # append bbox\n",
    "        out = torch.cat((out, bboxes), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        # append bbox\n",
    "        # print(\"shape\",product.shape)\n",
    "        # out = np.append(out, bbox.cpu(), axis=1)\n",
    "        print(f\"Output shape: {out.shape}\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQkhbZucR7eZ",
    "outputId": "da9313ce-1c8c-41e1-c69b-8c2c9b4fc254"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
      "tensor([[285.0400,  23.2900, 139.7400, 123.1100],\n",
      "        [285.0400,  23.2900, 139.7400, 123.1100]])\n",
      "Output shape: torch.Size([2, 1028])\n",
      "tensor([[ 3.9057e-04,  1.1195e-03,  3.7989e-03,  ...,  2.3290e+01,\n",
      "          1.3974e+02,  1.2311e+02],\n",
      "        [-7.2597e-04,  2.1368e-03,  3.9330e-03,  ...,  2.3290e+01,\n",
      "          1.3974e+02,  1.2311e+02]])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
      "tensor([[183.4000,  68.3400, 126.8100,  98.8100],\n",
      "        [183.4000,  68.3400, 126.8100,  98.8100]])\n",
      "Output shape: torch.Size([2, 1028])\n",
      "tensor([[-2.8235e-03, -3.0453e-04, -6.6093e-03,  ...,  6.8340e+01,\n",
      "          1.2681e+02,  9.8810e+01],\n",
      "        [-1.7187e-03, -4.3159e-04, -6.4131e-03,  ...,  6.8340e+01,\n",
      "          1.2681e+02,  9.8810e+01]])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([2, 1024]), Bbox shape: torch.Size([1, 4])\n",
      "tensor([[394.1600, 269.7600, 237.4600, 117.2900],\n",
      "        [394.1600, 269.7600, 237.4600, 117.2900]])\n",
      "Output shape: torch.Size([2, 1028])\n",
      "tensor([[-1.0636e-03,  2.6744e-04,  4.3551e-04,  ...,  2.6976e+02,\n",
      "          2.3746e+02,  1.1729e+02],\n",
      "        [-1.0225e-03, -4.7226e-04, -2.1107e-03,  ...,  2.6976e+02,\n",
      "          2.3746e+02,  1.1729e+02]])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024]), Bbox shape: torch.Size([1, 4])\n",
      "tensor([[ 90.9500, 158.2600, 196.1000, 152.0500]])\n",
      "Output shape: torch.Size([1, 1028])\n",
      "tensor([[ 1.4302e-03,  1.3224e-03, -2.1935e-03,  ...,  1.5826e+02,\n",
      "          1.9610e+02,  1.5205e+02]])\n",
      "Image shape: torch.Size([1, 1024]), Text shape: torch.Size([1, 1024]), Bbox shape: torch.Size([1, 4])\n",
      "tensor([[ 86.2600, 112.8500, 172.9300, 123.0000]])\n",
      "Output shape: torch.Size([1, 1028])\n",
      "tensor([[-4.3266e-03,  8.4821e-04, -7.8649e-03,  ...,  1.1285e+02,\n",
      "          1.7293e+02,  1.2300e+02]])\n"
     ]
    }
   ],
   "source": [
    "dataset = RefCOCOg('.', 'val')\n",
    "for i in range(5):\n",
    "  x = dataset[i]\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKroxpH0qnrG"
   },
   "source": [
    "## Our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_VKW0jTYc4LR",
    "outputId": "0f410a60-8f3c-422f-843c-cedf8af28caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n",
      "zsh:1: no matches found: gym[box2d]==0.17.*\n"
     ]
    }
   ],
   "source": [
    "# !apt-get install -y xvfb python-opengl swig x11-utils\n",
    "# !pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvvaS0iDl4cS",
    "outputId": "ce39f6e2-bb44-4730-d7c9-a7db2f2eb712"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164 112 284 278]\n",
      "[255  76  97 270]\n",
      "[433  23 167  55]\n",
      "[139 151 203 198]\n",
      "[240 252 500   4]\n",
      "[190  83 465 233]\n",
      "[369 207 107 218]\n",
      "[119  90 132 261]\n",
      "[ 56 169 101 117]\n",
      "[361 234 179  13]\n"
     ]
    }
   ],
   "source": [
    "from gym import spaces\n",
    "import numpy as np\n",
    "test = spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([499, 299, 500, 300]), dtype=int)\n",
    "# test = spaces.Box([500, 300, 500, 300], dtype=int)\n",
    "for i in range(10):\n",
    "  print(test.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_kBbwDrR8cQ"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "# class syntax\n",
    "class actions(Enum):\n",
    "  ACT_RT = 0 #Right\n",
    "  ACT_LT = 1 #Left\n",
    "  ACT_UP = 2 #Up\n",
    "  ACT_DN = 3 #Down\n",
    "  ACT_TA = 4 #Taller\n",
    "  ACT_FA = 5 #Fatter\n",
    "  ACT_SR = 6 #Shorter\n",
    "  ACT_TH = 7 #Thiner\n",
    "  ACT_TR = 8 #Trigger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "id": "DU5sU5s2cc24",
    "outputId": "7bf756ed-8186-448b-ee0e-baebcf6fcfe1"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3328d72bc599>\"\u001b[0;36m, line \u001b[0;32m72\u001b[0m\n\u001b[0;31m    self._target_location = #TODO: init with true bbox\u001b[0m\n\u001b[0m                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "class VisualGroundingEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "    def __init__(self, width, height, move_factor=0.2, scale_factor=0.1, render_mode=None):\n",
    "        self.width = width  # The width of the image\n",
    "        self.height = height  # The height of the image\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        self.move_factor = move_factor\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "        # Observations are dictionaries with the agent's and the target's location.\n",
    "        # Each location is encoded as an element of {0, ..., `size`}^2, i.e. MultiDiscrete([size, size]).\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([self.width-1, self.height, self.width, self.height]), dtype=int),\n",
    "                \"target\": spaces.Box(low=np.array([0, 0, 1, 1]), high=np.array([self.width-1, self.height, self.width, self.height]), dtype=int),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 9 actions, corresponding to \"right\", \"up\", \"left\", \"down\", \"v-shrink\", \"v-stretch\", \"h-shrink\", \"h-stretch\", \"confirm\"\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "\n",
    "        \"\"\"\n",
    "        The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "\n",
    "        #TODO: later when current data is available\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),\n",
    "            1: np.array([0, 1]),\n",
    "            2: np.array([-1, 0]),\n",
    "            3: np.array([0, -1]),\n",
    "        }\n",
    "\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        \"\"\"\n",
    "        If human-rendering is used, `self.window` will be a reference\n",
    "        to the window that we draw to. `self.clock` will be a clock that is used\n",
    "        to ensure that the environment is rendered at the correct framerate in\n",
    "        human-mode. They will remain `None` until human-mode is used for the\n",
    "        first time.\n",
    "        \"\"\"\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}\n",
    "\n",
    "    def _get_info(self):\n",
    "        # return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "        # TODO: maybe return current history of movement\n",
    "        pass\n",
    "\n",
    "    def reset(self, true_bbox: np.array, seed=None, options=None):\n",
    "        self.x1 = 0\n",
    "        self.y1 = 0\n",
    "        self.bbox_width = self.width\n",
    "        self.bbox_height = self.height\n",
    "\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose the agent's location uniformly at random\n",
    "        self._agent_location = np.array([0,0, self.width, self.height])\n",
    "\n",
    "        # We will sample the target's location randomly until it does not coincide with the agent's location\n",
    "        self._target_location = #TODO: init with true bbox\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def _update_bbox(self, action):\n",
    "      ALPHA = 0.2\n",
    "      BETA  = 0.1\n",
    "      x2 = self.x1 + self.bbox_width\n",
    "      y2 = self.y1 + self.bbox_height\n",
    "      assert action >= actions.ACT_RT and action <= actions.ACT_TR\n",
    "      self.action_history.append(action)\n",
    "\n",
    "      if action <= actions.ACT_DN:\n",
    "        delta_w = int(ALPHA * self.bbox_width())\n",
    "        delta_h = int(ALPHA * self.bbox_height())\n",
    "      else:\n",
    "        delta_w = int(BETA * self.bbox_width())\n",
    "        delta_h = int(BETA * self.bbox_height())\n",
    "\n",
    "      # PREVENT_STUCK:\n",
    "      if (delta_h == 0):\n",
    "        delta_h = 1\n",
    "      if (delta_w == 0):\n",
    "        delta_w = 1\n",
    "\n",
    "      #Do the corresponding action to the window\n",
    "      if action == actions.ACT_RT:\n",
    "        self.x1 += delta_w\n",
    "        x2 += delta_w\n",
    "      elif action == actions.ACT_LT:\n",
    "        self.x1 -= delta_w\n",
    "        x2 -= delta_w\n",
    "      elif action == actions.ACT_UP:\n",
    "        self.y1 -= delta_h\n",
    "        y2 -= delta_h\n",
    "      elif action == actions.ACT_DN:\n",
    "        self.y1 += delta_h\n",
    "        y2 += delta_h\n",
    "      elif action == actions.ACT_TA:\n",
    "        self.y1 -= delta_h\n",
    "        y2 += delta_h\n",
    "      elif action == actions.ACT_FA:\n",
    "        self.x1 -= delta_w\n",
    "        x2 += delta_w\n",
    "      elif action == actions.ACT_SR:\n",
    "        self.y1 += delta_h\n",
    "        y2 -= delta_h\n",
    "      elif action == actions.ACT_TH:\n",
    "        self.x1 += delta_w\n",
    "        x2 -= delta_w\n",
    "      elif action == actions.ACT_TR:\n",
    "        pass\n",
    "      else:\n",
    "        raise NotImplemented\n",
    "\n",
    "      # ensure bbox inside image\n",
    "      if self.x1 < 0:\n",
    "        self.x1 = 0\n",
    "      if self.y1 < 0:\n",
    "        self.y1 = 0\n",
    "      if self.x2 >= self.image_width:\n",
    "        self.x2 = self.image_width - 1\n",
    "      if self.y2 >= self.image_height:\n",
    "        self.y2 = self.image_height - 1\n",
    "      # ret x,y,w,h\n",
    "      return  self.x1, self.y1, x2-self.x1, y2-self.y1\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "            # Map the action (element of {0,1,2,3}) to the direction we walk in\n",
    "            self._agent_location = self._update_bbox(action)\n",
    "            # An episode is done iff the agent has reached the target\n",
    "            terminated = np.array_equal(self._agent_location, self._target_location) #TODO: or quite close\n",
    "            reward = 1 if terminated else 0  # TODO: change reward\n",
    "            observation = self._get_obs()\n",
    "            info = self._get_info()\n",
    "\n",
    "            if self.render_mode == \"human\":\n",
    "                self._render_frame()\n",
    "\n",
    "            return observation, reward, terminated, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xfi4D9z39mA7"
   },
   "outputs": [],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id='DeepLearningProject/VisualGrounding-v0',\n",
    "    entry_point='DeepLearningProject.envs:VisualGrounding',\n",
    "    max_episode_steps=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeSH5dic9zqt"
   },
   "outputs": [],
   "source": [
    "#TODO: move everything in a repo, but leave here for knowledge\n",
    "\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    name=\"DeepLearningProject\",\n",
    "    version=\"0.0.1\",\n",
    "    install_requires=[\"gym==0.26.0\", \"pygame==2.1.0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRUOg-oe-Ig5"
   },
   "outputs": [],
   "source": [
    "import gym_examples\n",
    "env = gym.make('DeepLearningProject/VisualGrounding-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKRUOZc6baAh"
   },
   "source": [
    "### Network, Agent and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31mKYCXhZyZf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxCoa4TUZ0L3"
   },
   "outputs": [],
   "source": [
    "#TODO: change\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_features, n_actions, features=24):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # multi layer perceptron\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_features, features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features, features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 2, features * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 4, features * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features * 2, features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sd7GcGt6Z4kO"
   },
   "outputs": [],
   "source": [
    "# create a subclass of Tuple with named attributes representing experience\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "\n",
    "        # represent the buffer as a deque\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "\n",
    "        # add the current experience to the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        # sample an index for each element in the batch\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        # extract experience entries for each element in the batch\n",
    "        # each value returned by zip is a list of length batch_size\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "\n",
    "        # return results as numpy arrays\n",
    "        return np.array(states), \\\n",
    "               np.array(actions), \\\n",
    "               np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), \\\n",
    "               np.array(next_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MomYFhu3Z6fX"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "\n",
    "        # restarts the environment and reset the accumulated reward\n",
    "        self.state = self.env.reset().astype(np.float32)\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "\n",
    "        # no need to create a computational graph when gathering experience\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # will contain the total reward for the episode if the episode ends\n",
    "            # or None otherwise\n",
    "            done_reward = None\n",
    "\n",
    "            # sample the action randomly with probability epsilon\n",
    "            if np.random.random() < epsilon:\n",
    "                action = self.env.action_space.sample()\n",
    "\n",
    "            # otherwise, select action based on qvalues\n",
    "            else:\n",
    "\n",
    "                # creates a batch made of a single state\n",
    "                # state_a = np.array([self.state], copy=False)\n",
    "                state_tensor = torch.tensor(self.state).unsqueeze(0).to(device)\n",
    "\n",
    "                # get qvalues and select the index of the maximum\n",
    "                q_values = net(state_tensor)\n",
    "                _, selected_action = torch.max(q_values, dim=1)\n",
    "                action = int(selected_action.item())\n",
    "\n",
    "            # perform a step in the environment\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            new_state = new_state.astype(np.float32)\n",
    "            self.total_reward += reward\n",
    "\n",
    "            # save the new experience\n",
    "            exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "            self.exp_buffer.append(exp)\n",
    "\n",
    "            # registers the current state\n",
    "            self.state = new_state\n",
    "\n",
    "            # Gets the current representation of the environment\n",
    "            # current_rgb_image = self.env.render(mode='rgb_array')\n",
    "\n",
    "            # if the episode is finished, reset the environment\n",
    "            if is_done:\n",
    "                done_reward = self.total_reward\n",
    "                self._reset()\n",
    "\n",
    "            return done_reward #, current_rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJQe0pCDZ8gJ"
   },
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, target_net, device=\"cpu\"):\n",
    "\n",
    "    # unpack the batch\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    #states_v = torch.tensor(np.array(states, copy=False)).to(device)\n",
    "    #next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)\n",
    "\n",
    "    # transform the batch elements to tensors\n",
    "    states_v = torch.from_numpy(states).to(device)\n",
    "    next_states_v = torch.from_numpy(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    # infer the qvalues for the states\n",
    "    state_qvalues = net(states_v)\n",
    "\n",
    "    # extract the qvalues for the action that was selected\n",
    "    state_action_qvalues = state_qvalues.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # compute the qvalues for the next states using the target DQN\n",
    "        next_state_qvalues = target_net(next_states_v)\n",
    "\n",
    "        # extract the maximum one\n",
    "        next_state_max_qvalue = next_state_qvalues.max(dim=1)[0]\n",
    "\n",
    "        # if the next state refers to an ended episode, it has no value\n",
    "        next_state_max_qvalue[done_mask] = 0.0\n",
    "\n",
    "        next_state_max_qvalue = next_state_max_qvalue.detach()\n",
    "\n",
    "    # Computes the expected qvalue using the Bellman equation\n",
    "    expected_state_action_qvalues = rewards_v + GAMMA * next_state_max_qvalue\n",
    "\n",
    "    # Penalizes the DQN for inferring a qvalue different from the one\n",
    "    # computed with the target DQN using the Bellman equation\n",
    "    return nn.MSELoss()(state_action_qvalues, expected_state_action_qvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMWB3mAZcIaz"
   },
   "source": [
    "### Train Loop and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLizKhHVZ-Sj"
   },
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"\"\n",
    "\n",
    "# we terminate training if the model on average balances the pole\n",
    "# for at least 195 steps\n",
    "MEAN_REWARD_BOUND = 195\n",
    "\n",
    "# discount factor\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# size of the replay buffer\n",
    "REPLAY_SIZE = 10000\n",
    "\n",
    "# warmup frames for the replay buffer\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# frequency for transferring weights from the actor DQN to the target DQN\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "\n",
    "# epsilon\n",
    "EPSILON_DECAY_LAST_FRAME = 15000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01\n",
    "\n",
    "def train(net, target_net, env, buffer, agent, device, writer):\n",
    "\n",
    "    # epsilon starts from the initial value and is then annealed\n",
    "    epsilon = EPSILON_START\n",
    "\n",
    "    # instantiate the optimizer. Note that target_net is not optimized\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    # frame idx and time at which the last episode ended\n",
    "    ts_frame = 0\n",
    "    ts = time.time()\n",
    "    best_m_reward = None\n",
    "\n",
    "    while True:\n",
    "        frame_idx += 1\n",
    "\n",
    "        # compute the current epsilon with linear annealing\n",
    "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "        # perform a step in the environment to gather experience\n",
    "        # reward, rbg_image = agent.play_step(net, epsilon, device=device)\n",
    "        reward = agent.play_step(net, epsilon, device=device)\n",
    "\n",
    "        # if the current episode has ended\n",
    "        if reward is not None:\n",
    "\n",
    "            # register the current total reward\n",
    "            total_rewards.append(reward)\n",
    "\n",
    "            # compute training speed\n",
    "            speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "            ts_frame = frame_idx\n",
    "            ts = time.time()\n",
    "\n",
    "            # compute the mean reward over the last 100 episodes\n",
    "            m_reward = np.mean(total_rewards[-100:])\n",
    "            print(\"%d: done %d games, reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "                frame_idx, len(total_rewards), m_reward, epsilon, speed\n",
    "            ))\n",
    "\n",
    "            # log values\n",
    "            writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "            writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "            writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
    "            writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "\n",
    "            # update best rewards\n",
    "            if best_m_reward is None or best_m_reward < m_reward:\n",
    "                #torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
    "                if best_m_reward is not None:\n",
    "                    print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
    "\n",
    "                best_m_reward = m_reward\n",
    "\n",
    "            # stop training when a certain reward is achieved\n",
    "            if m_reward > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d frames!\" % frame_idx)\n",
    "                break\n",
    "\n",
    "        # continue to collect experience until the warmup finishes\n",
    "        if len(buffer) < REPLAY_START_SIZE:\n",
    "            continue\n",
    "\n",
    "        # at regular intervals load the weights of the actor DQN into the target DQN\n",
    "        if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "            target_net.load_state_dict(net.state_dict())\n",
    "\n",
    "        # perform an optimization step\n",
    "        optimizer.zero_grad()\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        loss_t = calc_loss(batch, net, target_net, device=device)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHidO2pRaAHK"
   },
   "outputs": [],
   "source": [
    "def save_episode(net, env, agent, device, writer):\n",
    "\n",
    "    from IPython import display as ipythondisplay\n",
    "    from pyvirtualdisplay import Display\n",
    "    display = Display(visible=0, size=(400, 300))\n",
    "    display.start()\n",
    "\n",
    "    # reset the environment\n",
    "    agent._reset()\n",
    "    all_frames = []\n",
    "\n",
    "    reward = None\n",
    "\n",
    "    # play an episode\n",
    "    while reward is None:\n",
    "\n",
    "        reward, rgb_image = agent.play_step(net, epsilon=0.0, device=device)\n",
    "\n",
    "        # change fromat from (H, W, C) to (C, H, W) and saves the image\n",
    "        rgb_image = torch.from_numpy(np.copy(rgb_image)).permute(2, 0, 1)\n",
    "        all_frames.append(rgb_image)\n",
    "\n",
    "    # video must be put into (batch, time, C, H, W) format to be saved\n",
    "    video = torch.stack(all_frames, dim=0).unsqueeze(0)\n",
    "    # save the video\n",
    "    writer.add_video(\"sample_episode\", video, global_step=0, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9shkETtGaFl8"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    # build the environment\n",
    "    env = gym.make(DEFAULT_ENV_NAME)\n",
    "\n",
    "    # create actor and target DQN models\n",
    "    net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    target_net = DQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "\n",
    "    # initialize the logger\n",
    "    writer = SummaryWriter(log_dir=\"runs\")\n",
    "    print(net)\n",
    "\n",
    "    # instantiate the experience buffer and the agent that collects experience\n",
    "    buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "    agent = Agent(env, buffer)\n",
    "\n",
    "    # train the network\n",
    "    train(net, target_net, env, buffer, agent, device, writer)\n",
    "\n",
    "    # save a sample episode\n",
    "    # save_episode(net, env, agent, device, writer)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCTbsP1PaIA8"
   },
   "outputs": [],
   "source": [
    "!rm -r runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FZ2VopqaJzn"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDDKIPXZaOjp"
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CU0-J0Tt-qY"
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.subplot(2, 2, 1)\n",
    "# plt.plot(logs[\"reward\"])\n",
    "# plt.title(\"training rewards (average)\")\n",
    "# plt.subplot(2, 2, 2)\n",
    "# plt.plot(logs[\"step_count\"])\n",
    "# plt.title(\"Max step count (training)\")\n",
    "# plt.subplot(2, 2, 3)\n",
    "# plt.plot(logs[\"eval reward (sum)\"])\n",
    "# plt.title(\"Return (test)\")\n",
    "# plt.subplot(2, 2, 4)\n",
    "# plt.plot(logs[\"eval step_count\"])\n",
    "# plt.title(\"Max step count (test)\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeprl-MQ4pjQ9z-py3.11",
   "language": "python",
   "name": "deeprl-mq4pjq9z-py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
